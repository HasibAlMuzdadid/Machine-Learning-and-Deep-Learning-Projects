{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emojify sentences using word embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Q5xrjzKe5NslkgouczElCGu7_cfkNwJw",
      "authorship_tag": "ABX9TyPCH9z/1XifTx8FvYHBibuz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HasibAlMuzdadid/Machine-Learning-and-Deep-Learning-Projects/blob/main/emojify%20sentences%20using%20word%20embedding/emojify_sentences_using_word_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Emojify Sentences using Word Embedding**\n",
        "\n",
        "We will build a model using word embeddings."
      ],
      "metadata": {
        "id": "32I6A5cpR8XK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji==1.6.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGXepVdoZ_gD",
        "outputId": "e0307431-0bfd-401c-b75e-089467ede94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji==1.6.3 in /usr/local/lib/python3.7/dist-packages (1.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1uMrQiLRnJK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import emoji\n",
        "import csv\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "\n",
        "def read_csv(filename = \"data/emojify_data.csv\"):\n",
        "    phrase = []\n",
        "    emoji = []\n",
        "\n",
        "    with open (filename) as csvDataFile:\n",
        "        csvReader = csv.reader(csvDataFile)\n",
        "\n",
        "        for row in csvReader:\n",
        "            phrase.append(row[0])\n",
        "            emoji.append(row[1])\n",
        "\n",
        "    X = np.asarray(phrase)\n",
        "    Y = np.asarray(emoji, dtype=int)\n",
        "\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "BVglRzqubub_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_dictionary = {#\"0\": \":red_heart:\",    # :heart: prints a black instead of red heart depending on the font\n",
        "                    \"0\": \"\\u2764\\ufe0f\",\n",
        "                    \"1\": \":baseball:\",\n",
        "                    \"2\": \":smile:\",\n",
        "                    \"3\": \":disappointed:\",\n",
        "                    \"4\": \":fork_and_knife:\"}\n",
        "\n",
        "def label_to_emoji(label):\n",
        "\n",
        "    # Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
        "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
        " "
      ],
      "metadata": {
        "id": "QID4SgHLcnD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y"
      ],
      "metadata": {
        "id": "epPHNGFekHCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()"
      ],
      "metadata": {
        "id": "SVpal0Zo8tCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map"
      ],
      "metadata": {
        "id": "l7Dnpm6tpGYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, Y, W, b, word_to_vec_map):\n",
        "\n",
        "    # Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of model over the given set\n",
        "    \n",
        "    # Arguments:\n",
        "    # X -> input data containing sentences, numpy array of shape (m, None)\n",
        "    # Y -> labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
        "    \n",
        "    # Returns:\n",
        "    # pred -> numpy array of shape (m, 1) with your predictions\n",
        "\n",
        "    m = X.shape[0]\n",
        "    pred = np.zeros((m, 1))\n",
        "    any_word = list(word_to_vec_map.keys())[0]\n",
        "    # number of classes  \n",
        "    n_h = word_to_vec_map[any_word].shape[0] \n",
        "    \n",
        "    for j in range(m):                       # Loop over training examples\n",
        "        \n",
        "        # Split jth test example (sentence) into list of lower case words\n",
        "        words = X[j].lower().split()\n",
        "        \n",
        "        # Average words' vectors\n",
        "        avg = np.zeros((n_h,))\n",
        "        count = 0\n",
        "        for w in words:\n",
        "            if w in word_to_vec_map:\n",
        "                avg += word_to_vec_map[w]\n",
        "                count += 1\n",
        "        \n",
        "        if count > 0:\n",
        "            avg = avg / count\n",
        "\n",
        "        # Forward propagation\n",
        "        Z = np.dot(W, avg) + b\n",
        "        A = softmax(Z)\n",
        "        pred[j] = np.argmax(A)\n",
        "        \n",
        "    print(f\"Accuracy: {str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:])))}\")\n",
        "    \n",
        "    return pred"
      ],
      "metadata": {
        "id": "pPNbwyiU86FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_predictions(X, pred):\n",
        "    print()\n",
        "    for i in range(X.shape[0]):\n",
        "        print(X[i], label_to_emoji(int(pred[i])))"
      ],
      "metadata": {
        "id": "gnPcKAs9_UXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_actu, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
        "    \n",
        "    df_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
        "    \n",
        "    df_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
        "    \n",
        "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
        "    #plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(df_confusion.columns))\n",
        "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
        "    plt.yticks(tick_marks, df_confusion.index)\n",
        "    #plt.tight_layout()\n",
        "    plt.ylabel(df_confusion.index.name)\n",
        "    plt.xlabel(df_confusion.columns.name)"
      ],
      "metadata": {
        "id": "IDC2Ho1LA_5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**\n",
        "\n",
        "We have a tiny dataset (X, Y) where:\n",
        "- X contains 127 sentences (strings)\n",
        "- Y contains an integer label between 0 and 4 corresponding to an emoji for each sentence\n",
        "\n",
        "\n",
        "The dataset is splitted between training (127 examples) and testing (56 examples).\n"
      ],
      "metadata": {
        "id": "yLoBnhhqaO-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading dataset\n",
        "\n",
        "X_train, Y_train = read_csv(\"/content/train_emoji.csv\")\n",
        "X_test, Y_test = read_csv(\"/content/tesss.csv\")"
      ],
      "metadata": {
        "id": "qRbjpHasaqFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxLen = len(max(X_train, key=len).split())"
      ],
      "metadata": {
        "id": "helEGFmDcSzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing sentences from X_train and corresponding labels from Y_train"
      ],
      "metadata": {
        "id": "e7BgbY1Jd3qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(10):\n",
        "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlXVn16qcgN7",
        "outputId": "b483db79-7cfd-4e39-f440-d81c2cfbc8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "never talk to me again üòû\n",
            "I am proud of your achievements üòÑ\n",
            "It is the worst day in my life üòû\n",
            "Miss you so much ‚ù§Ô∏è\n",
            "food is life üç¥\n",
            "I love you mum ‚ù§Ô∏è\n",
            "Stop saying bullshit üòû\n",
            "congratulations on your acceptance üòÑ\n",
            "The assignment is too long  üòû\n",
            "I want to go play ‚öæ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inputs and Outputs**\n",
        "* The input of the model is a string corresponding to a sentence \n",
        "* The output will be a probability vector of shape (1,5), (indicating that there are 5 emojis to choose from)\n",
        "* The (1,5) probability vector is passed to an argmax layer which extracts the index of the emoji with the highest probability\n",
        "\n",
        "\n",
        "**One-hot Encoding**\n",
        "* To get our labels into a format suitable for training a softmax classifier, converting $Y$ from its current shape  $(m, 1)$ into a \"one-hot representation\" $(m, 5)$, \n",
        "    * Each row is a one-hot vector giving the label of one example\n",
        "    * Here, `Y_oh` stands for \"Y-one-hot\" in the variable names `Y_oh_train` and `Y_oh_test`: "
      ],
      "metadata": {
        "id": "2GiKIlukjcVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
        "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
      ],
      "metadata": {
        "id": "YW9uGVxvkAup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 50\n",
        "print(f\"Sentence '{X_train[idx]}' has label index {Y_train[idx]} which is emoji {label_to_emoji(Y_train[idx])}\")\n",
        "print(f\"Label index {Y_train[idx]} in one-hot encoding format is {Y_oh_train[idx]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dE-djlekOxV",
        "outputId": "acb4d2ec-5f89-415a-cdda-79a59e2c700b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 'I missed you' has label index 0 which is emoji ‚ù§Ô∏è\n",
            "Label index 0 in one-hot encoding format is [1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Emojifier**\n",
        "\n",
        "The first step is to:\n",
        "* Convert each word in the input sentence into their word vector representations\n",
        "* Take an average of the word vectors \n",
        "\n",
        "We'll use pre-trained 50-dimensional GloVe embeddings. \n",
        "\n"
      ],
      "metadata": {
        "id": "ijt5xWbeod1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the `word_to_vec_map` which contains all the vector representations\n",
        "\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(\"/content/drive/MyDrive/colab files/glove.6B.50d.txt\")"
      ],
      "metadata": {
        "id": "TYi1UKtFo1b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here:\n",
        "- `word_to_index`: dictionary mapping from words to their indices in the vocabulary \n",
        "    - (400,001 words with the valid indices ranging from 0 to 400,000)\n",
        "- `index_to_word`: dictionary mapping from indices to their corresponding words in the vocabulary\n",
        "- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation\n"
      ],
      "metadata": {
        "id": "SZUBTuHhpqGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"cucumber\"\n",
        "idx = 289846\n",
        "print(f\"the index of {word} in the vocabulary is {word_to_index[word]}\")\n",
        "print(f\"the {str(idx)}th word in the vocabulary is {index_to_word[idx]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHHHRy9LpyJy",
        "outputId": "c68d47cb-011c-4869-d6a9-59bccce188f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the index of cucumber in the vocabulary is 113317\n",
            "the 289846th word in the vocabulary is potatos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement `sentence_to_avg()`** \n",
        "\n",
        "We'll need to carry out two steps:\n",
        "\n",
        "1. Convert every sentence to lower-case, then split the sentence into a list of words. \n",
        "    * `X.lower()` and `X.split()` might be useful. üòâ\n",
        "2. For each word in the sentence, access its GloVe representation.\n",
        "    * Then take the average of all of these word vectors.\n",
        "    * We might use `numpy.zeros()`\n",
        "\n",
        "    * When creating the `avg` array of zeros, we'll want it to be a vector of the same shape as the other word vectors in the `word_to_vec_map`.  \n",
        "    * We can choose a word that exists in the `word_to_vec_map` and access its `.shape` field.\n",
        "\n",
        "\n",
        "We can use any one of the word vectors that we retrieved from the input `sentence` to find the shape of a word vector."
      ],
      "metadata": {
        "id": "vG9WluHIqhhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence to avg\n",
        "\n",
        "def sentence_to_avg(sentence, word_to_vec_map):\n",
        "    \n",
        "    # Arguments:\n",
        "    # sentence -> string, one training example from X\n",
        "    # word_to_vec_map -> dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    \n",
        "    # Returns:\n",
        "    # avg -> average vector encoding information about the sentence, numpy-array of shape (J,) where J can be any number\n",
        "\n",
        "\n",
        "    # Get a valid word contained in the word_to_vec_map \n",
        "    any_word = list(word_to_vec_map.keys())[0]\n",
        "    \n",
        "    # Step 1: Split sentence into list of lower case words \n",
        "    words = (sentence.lower()).split()\n",
        "    \n",
        "    # Initialize the average word vector that should have the same shape as your word vectors\n",
        "    avg = np.zeros((word_to_vec_map['a'].shape))\n",
        "    \n",
        "    # Initialize count to 0\n",
        "    count = 0\n",
        "    \n",
        "    # Step 2: average the word vectors. We can loop over the words in the list \"words\"\n",
        "    for w in words:\n",
        "        # Check that word exists in word_to_vec_map\n",
        "        if w in word_to_vec_map:\n",
        "            avg += word_to_vec_map[w]\n",
        "            # Increment count\n",
        "            count +=1\n",
        "          \n",
        "    if count > 0:\n",
        "        # Get the average. But only if count > 0\n",
        "        avg = avg / count\n",
        "    \n",
        "    \n",
        "    return avg"
      ],
      "metadata": {
        "id": "clFZ8Bc4r4Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement The Model**\n",
        "\n",
        "We have all the pieces to finish implementing the `model()` function! \n",
        "After using `sentence_to_avg()` we need to:\n",
        "* Pass the average through forward propagation\n",
        "* Compute the cost\n",
        "* Backpropagate to update the softmax parameters\n",
        "\n",
        "\n",
        "* The equations we need to implement in the forward pass and to compute the cross-entropy cost are below:\n",
        " \n",
        "\n",
        "$$ z^{(i)} = Wavg^{(i)} + b$$\n",
        "\n",
        "$$ a^{(i)} = softmax(z^{(i)})$$\n",
        "\n",
        "$$ \\mathcal{L}^{(i)} = - \\sum_{k = 0}^{n_y - 1} Y_{oh,k}^{(i)} * log(a^{(i)}_k)$$\n",
        "\n"
      ],
      "metadata": {
        "id": "yojlY302sqKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "\n",
        "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
        "    \n",
        "    # Arguments:\n",
        "    # X -> input data, numpy array of sentences as strings of shape (m, 1)\n",
        "    # Y -> labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
        "    # word_to_vec_map -> dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    # learning_rate -> learning_rate for the stochastic gradient descent algorithm\n",
        "    # num_iterations -> number of iterations\n",
        "    \n",
        "    # Returns:\n",
        "    # pred -> vector of predictions, numpy-array of shape (m, 1)\n",
        "    # W -> weight matrix of the softmax layer of shape (n_y, n_h)\n",
        "    # b -> bias of the softmax layer of shape (n_y,)\n",
        "\n",
        "    \n",
        "    # Get a valid word contained in the word_to_vec_map \n",
        "    any_word = list(word_to_vec_map.keys())[0]\n",
        "    \n",
        "    # Define number of training examples\n",
        "    m = Y.shape[0]                             # number of training examples\n",
        "    n_y = len(np.unique(Y))                    # number of classes  \n",
        "    n_h = word_to_vec_map[any_word].shape[0]   # dimensions of the GloVe vectors \n",
        "    \n",
        "    # Initialize parameters using Xavier initialization\n",
        "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
        "    b = np.zeros((n_y,))\n",
        "    \n",
        "    # Convert Y to Y_onehot with n_y classes\n",
        "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
        "    \n",
        "    # Optimization loop\n",
        "    for t in range(num_iterations): # Loop over the number of iterations\n",
        "        for i in range(m):          # Loop over the training examples\n",
        "            \n",
        "            # Average the word vectors of the words from the i'th training example\n",
        "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
        "\n",
        "            # Forward propagate the avg through the softmax layer \n",
        "            z = np.dot(W, avg) + b\n",
        "            a = softmax(z)\n",
        "\n",
        "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
        "            cost = - np.sum(Y_oh[i] * np.log(a))\n",
        "            \n",
        "            # Compute gradients \n",
        "            dz = a - Y_oh[i]\n",
        "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
        "            db = dz\n",
        "\n",
        "            # Update parameters with Stochastic Gradient Descent\n",
        "            W = W - learning_rate * dW\n",
        "            b = b - learning_rate * db\n",
        "        \n",
        "        if t % 100 == 0:\n",
        "            print(f\"Epoch: {str(t)} --- cost = {str(cost)}\")\n",
        "            pred = predict(X, Y, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
        "\n",
        "    return pred, W, b"
      ],
      "metadata": {
        "id": "6C6QKhEJ7nVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model and learning the softmax parameters (W, b)"
      ],
      "metadata": {
        "id": "XW8VlUwc9vJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzwT1eJF9rRH",
        "outputId": "c63b6d12-acde-494a-bb07-f9da6be661c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 --- cost = 1.514551646342196\n",
            "Accuracy: 0.45454545454545453\n",
            "Epoch: 100 --- cost = 0.07193276890237922\n",
            "Accuracy: 0.9318181818181818\n",
            "Epoch: 200 --- cost = 0.041940843875099776\n",
            "Accuracy: 0.946969696969697\n",
            "Epoch: 300 --- cost = 0.032938930162471305\n",
            "Accuracy: 0.9696969696969697\n",
            "[[3.]\n",
            " [2.]\n",
            " [3.]\n",
            " [0.]\n",
            " [4.]\n",
            " [0.]\n",
            " [3.]\n",
            " [2.]\n",
            " [3.]\n",
            " [1.]\n",
            " [3.]\n",
            " [3.]\n",
            " [1.]\n",
            " [3.]\n",
            " [2.]\n",
            " [3.]\n",
            " [2.]\n",
            " [3.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [0.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [1.]\n",
            " [4.]\n",
            " [3.]\n",
            " [3.]\n",
            " [4.]\n",
            " [0.]\n",
            " [3.]\n",
            " [4.]\n",
            " [2.]\n",
            " [0.]\n",
            " [3.]\n",
            " [2.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [2.]\n",
            " [2.]\n",
            " [0.]\n",
            " [2.]\n",
            " [3.]\n",
            " [0.]\n",
            " [3.]\n",
            " [2.]\n",
            " [4.]\n",
            " [3.]\n",
            " [0.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [4.]\n",
            " [2.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [3.]\n",
            " [4.]\n",
            " [4.]\n",
            " [2.]\n",
            " [2.]\n",
            " [1.]\n",
            " [2.]\n",
            " [0.]\n",
            " [3.]\n",
            " [2.]\n",
            " [2.]\n",
            " [0.]\n",
            " [3.]\n",
            " [3.]\n",
            " [1.]\n",
            " [2.]\n",
            " [1.]\n",
            " [2.]\n",
            " [2.]\n",
            " [4.]\n",
            " [3.]\n",
            " [3.]\n",
            " [2.]\n",
            " [4.]\n",
            " [0.]\n",
            " [0.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [2.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [0.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [3.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [4.]\n",
            " [1.]\n",
            " [1.]\n",
            " [3.]\n",
            " [3.]\n",
            " [4.]\n",
            " [1.]\n",
            " [2.]\n",
            " [1.]\n",
            " [1.]\n",
            " [3.]\n",
            " [1.]\n",
            " [0.]\n",
            " [4.]\n",
            " [0.]\n",
            " [3.]\n",
            " [3.]\n",
            " [4.]\n",
            " [4.]\n",
            " [1.]\n",
            " [4.]\n",
            " [3.]\n",
            " [0.]\n",
            " [2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance on test Set "
      ],
      "metadata": {
        "id": "fTI7Wrci98d9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training set:\")\n",
        "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
        "print(\"Test set:\")\n",
        "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4-Isc-0-Ag4",
        "outputId": "30f99c77-637a-4426-ea17-e7d3b57318b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set:\n",
            "Accuracy: 0.9772727272727273\n",
            "Test set:\n",
            "Accuracy: 0.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random guessing would have had 20% accuracy. Given that there are 5 classes. (1/5 = 20%). This is pretty good performance after training on only 127 examples. \n",
        "\n",
        "In the training set, the algorithm saw the sentence \"I love you.\" with the label ‚ù§Ô∏è. \n",
        "The word \"cherish\" does not appear in the training set, let's see what happens if we write \"I cherish you\"."
      ],
      "metadata": {
        "id": "GjOQo5vp-qdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_my_sentences = np.array([\"i cherish you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
        "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
        "\n",
        "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
        "print_predictions(X_my_sentences, pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQHxsSOp_ClE",
        "outputId": "09cbc104-eb6a-46d1-cd6e-c3c7e62ad3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8333333333333334\n",
            "\n",
            "i cherish you ‚ù§Ô∏è\n",
            "i love you ‚ù§Ô∏è\n",
            "funny lol üòÑ\n",
            "lets play with a ball ‚öæ\n",
            "food is ready üç¥\n",
            "not feeling happy üòÑ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because *adore* has a similar embedding as *love*, the algorithm has generalized correctly even to a word it has never seen before. Words such as *heart*, *dear*, *beloved* or *adore* have embedding vectors similar to *love*. \n",
        "\n",
        "\n",
        "**Word Ordering isn't Considered in This Model**\n",
        "\n",
        "The model doesn't get the following sentence correct:\"not feeling happy\". This algorithm ignores word ordering, so is not good at understanding phrases like \"not happy\". \n",
        "\n",
        "**Confusion Matrix**\n",
        "* Printing the confusion matrix can also help understand which classes are more difficult for the model. \n",
        "* A confusion matrix shows how often an example whose label is one class (\"actual\" class) is mislabeled by the algorithm with a different class (\"predicted\" class)."
      ],
      "metadata": {
        "id": "EDunB163_tL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_test.shape)\n",
        "print(f\"           {label_to_emoji(0)}   {label_to_emoji(1)}  {label_to_emoji(2)}   {label_to_emoji(3)}  {label_to_emoji(4)}\")\n",
        "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
        "plot_confusion_matrix(Y_test, pred_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "jo_U4DMDAUO3",
        "outputId": "fbfddf61-ad09-4bae-8d3d-f72d73ab410f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(56,)\n",
            "           ‚ù§Ô∏è   ‚öæ  üòÑ   üòû  üç¥\n",
            "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
            "Actual                                 \n",
            "0            6    0    0    1    0    7\n",
            "1            0    8    0    0    0    8\n",
            "2            1    0   17    0    0   18\n",
            "3            1    1    2   12    0   16\n",
            "4            0    0    1    0    6    7\n",
            "All          8    9   20   13    6   56\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model just averages all the word embedding vectors together without considering the order of words."
      ],
      "metadata": {
        "id": "Nph6rAqqBlA-"
      }
    }
  ]
}