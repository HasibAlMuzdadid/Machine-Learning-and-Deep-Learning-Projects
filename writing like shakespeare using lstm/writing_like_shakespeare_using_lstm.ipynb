{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgMJl7DtN85uLiurWrvaF9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HasibAlMuzdadid/Machine-Learning-and-Deep-Learning-Projects/blob/main/writing%20like%20shakespeare%20using%20lstm/writing_like_shakespeare_using_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Writing like Shakespeare using LSTM**\n",
        "\n",
        "\n",
        "A similar task to character-level text generation (but more complicated) is generating Shakespearean poems. We will use a collection of Shakespearean poems as dataset. Using LSTM cells, we can learn longer-term dependencies that span many characters in the text--e.g., where a character appearing somewhere a sequence can influence what should be a different character, much later in the sequence. \n",
        "\n"
      ],
      "metadata": {
        "id": "sYiZcMcsrALm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8uYMUh4qkJ6"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Model, load_model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "\n",
        "def build_data(text, Tx = 40, stride = 3):\n",
        "\n",
        "    # Create a training set by scanning a window of size Tx over the text corpus with stride 3.\n",
        "    \n",
        "    # Arguments:\n",
        "    # text -> string, corpus of Shakespearian poem\n",
        "    # Tx -> sequence length, number of time-steps (or characters) in one training example\n",
        "    # stride -> how much the window shifts itself while scanning\n",
        "    \n",
        "    # Returns:\n",
        "    # X -> list of training examples\n",
        "    # Y -> list of training labels\n",
        "  \n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    for i in range(0, len(text) - Tx, stride):\n",
        "        X.append(text[i: i + Tx])\n",
        "        Y.append(text[i + Tx])\n",
        "    \n",
        "    print(f\"number of training examples: {len(X)}\")\n",
        "    \n",
        "    return X, Y\n"
      ],
      "metadata": {
        "id": "mKWzSrwpvPbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorization(X, Y, n_x, char_indices, Tx = 40):\n",
        "\n",
        "    # Convert X and Y (lists) into arrays to be given to a recurrent neural network.\n",
        "    \n",
        "    # Arguments:\n",
        "    # X  \n",
        "    # Y  \n",
        "    # Tx -> integer, sequence length\n",
        "    \n",
        "    # Returns:\n",
        "    # x -> array of shape (m, Tx, len(chars))\n",
        "    # y -> array of shape (m, len(chars))\n",
        "\n",
        "    \n",
        "    m = len(X)\n",
        "    x = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
        "    y = np.zeros((m, n_x), dtype=np.bool)\n",
        "    for i, sentence in enumerate(X):\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[i, t, char_indices[char]] = 1\n",
        "        y[i, char_indices[Y[i]]] = 1\n",
        "        \n",
        "    return x, y "
      ],
      "metadata": {
        "id": "_0LnoPOTvvUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    out = np.random.choice(range(len(chars)), p = probas.ravel())\n",
        "    return out"
      ],
      "metadata": {
        "id": "JFYhD2nBwGvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def on_epoch_end(epoch, logs):\n",
        "  # Function invoked at end of each epoch. Prints generated text.\n",
        "  None\n",
        "\n",
        "\n",
        "print(\"Loading text data...\")\n",
        "text = io.open(\"/content/shakespeare.txt\", encoding='utf-8').read().lower()\n",
        "#print('corpus length:', len(text))\n",
        "\n",
        "Tx = 40\n",
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "#print('number of unique characters in the corpus:', len(chars))\n",
        "\n",
        "print(\"Creating training set...\")\n",
        "X, Y = build_data(text, Tx, stride = 3)\n",
        "print(\"Vectorizing training set...\")\n",
        "x, y = vectorization(X, Y, n_x = len(chars), char_indices = char_indices) \n",
        "print(\"Loading model...\")\n",
        "model = load_model(\"/content/model_shakespeare_kiank_350_epoch.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nWJugFtwXRI",
        "outputId": "7599c188-5602-4410-f4a1-e239340d17be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading text data...\n",
            "Creating training set...\n",
            "number of training examples: 31412\n",
            "Vectorizing training set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_output():\n",
        "    generated = ''\n",
        "    #sentence = text[start_index: start_index + Tx]\n",
        "    #sentence = '0'*Tx\n",
        "    usr_input = input(\"Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: \")\n",
        "    # zero pad the sentence to Tx characters.\n",
        "    sentence = ('{0:0>' + str(Tx) + '}').format(usr_input).lower()\n",
        "    generated += usr_input \n",
        "\n",
        "    sys.stdout.write(\"\\n\\nHere is your poem: \\n\\n\") \n",
        "    sys.stdout.write(usr_input)\n",
        "    for i in range(400):\n",
        "\n",
        "        x_pred = np.zeros((1, Tx, len(chars)))\n",
        "\n",
        "        for t, char in enumerate(sentence):\n",
        "            if char != '0':\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, temperature = 1.0)\n",
        "        next_char = indices_char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if next_char == '\\n':\n",
        "            continue"
      ],
      "metadata": {
        "id": "s3FrLzALyu3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A model has already been trained for ~1000 epochs on a collection of Shakespearean poems. \n",
        "\n",
        "Let's train the model for one more epoch. When it finishes training for an epoch, we can run `generate_output`, which will ask for an input (`<`40 characters). The poem will start with the input sentence and our RNN Shakespeare will complete the rest of the poem! For example, trying \"Forsooth this maketh no sense\" (without the quotation marks!). Depending on whether we include the space at the end, our results might also differ."
      ],
      "metadata": {
        "id": "BXIT2Uk32vfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4xNPt7g5ZEI",
        "outputId": "030d120b-8925-483c-b642-58029d1db470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "246/246 [==============================] - 88s 340ms/step - loss: 2.5371\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb8640a6f90>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_output()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQKlEN3T5orE",
        "outputId": "62a6726d-554a-40f1-acba-1674c4a327dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: Forsooth this maketh no sense\n",
            "\n",
            "\n",
            "Here is your poem: \n",
            "\n",
            "Forsooth this maketh no sensew,\n",
            "thy praise lend with girful onour womts imferlight,\n",
            "and faves, and eorhty forse though thou self\n",
            "but thy him thas thee but this sinch thoug, hit prosse.\n",
            "seand thou of deleck beauty thy behetse,\n",
            "lide astes that i to bothed me in dhow stand.\n",
            "thoe i ormom'ntence to mitafaon enceess.\n",
            "reciriks that were hadning of a bor leges,\n",
            "in that the by goed mancee part aiaking,\n",
            "that thee denerver and one of to"
          ]
        }
      ]
    }
  ]
}