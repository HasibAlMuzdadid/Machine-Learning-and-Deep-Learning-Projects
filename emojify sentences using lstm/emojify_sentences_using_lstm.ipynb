{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emojify sentences using lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1orevwSDMmBylOqBFvQaDC4r-vkXJqgbr",
      "authorship_tag": "ABX9TyNJVGo8vD5G0aNRuO0GXiyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HasibAlMuzdadid/Machine-Learning-and-Deep-Learning-Projects/blob/main/emojify%20sentences%20using%20lstm/emojify_sentences_using_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Emojify Sentences using LSTM**\n",
        " \n",
        "\n",
        "For building this model we will use a pre-trained word embeddings to represent words. We'll feed word embeddings into an LSTM and the LSTM will learn to predict the most appropriate emoji. This model will be able to account for word ordering. \n"
      ],
      "metadata": {
        "id": "TWVLoxN7cOl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji==1.6.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "App6wny7w35G",
        "outputId": "20378355-9aa6-49c8-cc2c-c94b9d0b0be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji==1.6.3 in /usr/local/lib/python3.7/dist-packages (1.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqoAwm9mLE0S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import csv\n",
        "import emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "\n",
        "def read_csv(filename = \"data/emojify_data.csv\"):\n",
        "    phrase = []\n",
        "    emoji = []\n",
        "\n",
        "    with open (filename) as csvDataFile:\n",
        "        csvReader = csv.reader(csvDataFile)\n",
        "\n",
        "        for row in csvReader:\n",
        "            phrase.append(row[0])\n",
        "            emoji.append(row[1])\n",
        "\n",
        "    X = np.asarray(phrase)\n",
        "    Y = np.asarray(emoji, dtype=int)\n",
        "\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "jCZ2LoyuwPjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_dictionary = {#\"0\": \":red_heart:\",    # :heart: prints a black instead of red heart depending on the font\n",
        "                    \"0\": \"\\u2764\\ufe0f\",\n",
        "                    \"1\": \":baseball:\",\n",
        "                    \"2\": \":smile:\",\n",
        "                    \"3\": \":disappointed:\",\n",
        "                    \"4\": \":fork_and_knife:\"}\n",
        "\n",
        "def label_to_emoji(label):\n",
        "\n",
        "    # Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
        "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)"
      ],
      "metadata": {
        "id": "mPOUKCOcwsr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map"
      ],
      "metadata": {
        "id": "yF2BxxZpyl4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y"
      ],
      "metadata": {
        "id": "Gq4sxb29EuM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**\n",
        "\n",
        "We have a tiny dataset (X, Y) where:\n",
        "- X contains 127 sentences (strings)\n",
        "- Y contains an integer label between 0 and 4 corresponding to an emoji for each sentence\n",
        "\n",
        "\n",
        "The dataset is splitted between training (127 examples) and testing (56 examples).\n"
      ],
      "metadata": {
        "id": "Qyr-Vqo6wIOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading dataset\n",
        "\n",
        "X_train, Y_train = read_csv(\"/content/train_emoji.csv\")\n",
        "X_test, Y_test = read_csv(\"/content/tesss.csv\")"
      ],
      "metadata": {
        "id": "p10GKdAfwJsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxLen = len(max(X_train, key=len).split())\n",
        "print(maxLen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHPskwQUxMx_",
        "outputId": "e834415f-21ff-4cf2-a993-30f9a686edfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing sentences from X_train and corresponding labels from Y_train"
      ],
      "metadata": {
        "id": "vKscwbMmxcIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(10):\n",
        "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHU2TuPExZel",
        "outputId": "e2832ee9-3bc6-428f-9af5-92b4119d35c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "never talk to me again üòû\n",
            "I am proud of your achievements üòÑ\n",
            "It is the worst day in my life üòû\n",
            "Miss you so much ‚ù§Ô∏è\n",
            "food is life üç¥\n",
            "I love you mum ‚ù§Ô∏è\n",
            "Stop saying bullshit üòû\n",
            "congratulations on your acceptance üòÑ\n",
            "The assignment is too long  üòû\n",
            "I want to go play ‚öæ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the pre-trained model which contains all the vector representations\n",
        "\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(\"/content/drive/MyDrive/colab files/glove.6B.50d.txt\")"
      ],
      "metadata": {
        "id": "HMz5tXpQyK6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keras and Mini-batching** \n",
        "\n",
        "We will train Keras using mini-batches. However, most deep learning frameworks require that all sequences in the same mini-batch have the **same length**. \n",
        "\n",
        "This is what allows vectorization to work: If we had a 3-word sentence and a 4-word sentence, then the computations needed for them are different (one takes 3 steps of an LSTM, one takes 4 steps) so it's just not possible to do them both at the same time.\n",
        "    \n",
        "#### Padding Handles Sequences of Varying Length\n",
        "The common solution to handling sequences of **different length** is to use padding. Specifically:\n",
        "   * Set a maximum sequence length. One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.\n",
        "   * Pad all sequences to have the same length. \n",
        "    \n",
        "**The Embedding Layer**\n",
        "\n",
        "In Keras, the embedding matrix is represented as a \"layer\". The embedding matrix maps word indices to embedding vectors.\n",
        "* The word indices are positive integers\n",
        "* The embedding vectors are dense vectors of fixed size\n",
        "* A \"dense\" vector is the opposite of a sparse vector. It means that most of its values are non-zero.  As a counter-example, a one-hot encoded vector is not \"dense\".\n",
        "\n",
        "The embedding matrix can be derived in two ways:\n",
        "* Training a model to derive the embeddings from scratch. \n",
        "* Using a pretrained embedding.\n",
        "    \n",
        "**Using and Updating Pre-trained Embeddings**\n",
        "\n",
        "We'll create an [Embedding()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer in Keras\n",
        "\n",
        "* We will initialize the Embedding layer with GloVe 50-dimensional vectors. \n",
        "\n",
        "\n",
        "Inputs and Outputs to the Embedding Layer :\n",
        "\n",
        "* The `Embedding()` layer's input is an integer matrix of size **(batch size, max input length)**. This input corresponds to sentences converted into lists of indices (integers). The largest integer (the highest word index) in the input should not be larger than the vocabulary size.\n",
        "* The embedding layer outputs an array of shape (batch size, max input length, dimension of word vectors).\n",
        "\n",
        "\n",
        "**Prepare Input Sentences**\n",
        "\n",
        "\n",
        "Implementing `sentences_to_indices` function which processes an array of sentences X and returns inputs to the embedding layer:\n",
        "\n",
        "* Convert each training sentences into a list of indices (the indices correspond to each word in the sentence)\n",
        "* Zero-pad all these lists so that their length is the length of the longest sentence\n"
      ],
      "metadata": {
        "id": "jgxjMqwZgVC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, val in enumerate([\"I\", \"like\", \"learning\"]):\n",
        "    print(idx, val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH6IWGgUstEo",
        "outputId": "07fe80be-baf2-45f5-c586-adb287dbe9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 I\n",
            "1 like\n",
            "2 learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences to Indices\n",
        "\n",
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "     \n",
        "    # Arguments:\n",
        "    # X -> array of sentences (strings) of shape (m, 1)\n",
        "    # word_to_index -> a dictionary containing the each word mapped to its index\n",
        "    # max_len -> maximum number of words in a sentence that we can assume every sentence in X is no longer than this \n",
        "    \n",
        "    # Returns:\n",
        "    # X_indices -> array of indices corresponding to words in the sentences from X of shape (m, max_len)\n",
        "\n",
        "    \n",
        "    m = X.shape[0]     # number of training examples\n",
        "\n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape \n",
        "    X_indices = np.zeros((m, max_len))\n",
        "    \n",
        "    for i in range(m):              # loop over training examples\n",
        "        \n",
        "        # Convert the ith training sentence in lower case and split is into words. We should get a list of words.\n",
        "        sentence_words = X[i].lower().split()\n",
        "        \n",
        "        # Initialize j to 0\n",
        "        j = 0\n",
        "        \n",
        "        # Loop over the words of sentence_words\n",
        "        for w in sentence_words:\n",
        "            # if w exists in the word_to_index dictionary\n",
        "            if w in word_to_index:\n",
        "                # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "                X_indices[i, j] = word_to_index[w]\n",
        "                # Increment j to j + 1\n",
        "                j =  j + 1\n",
        "    \n",
        "    return X_indices"
      ],
      "metadata": {
        "id": "EMaI3p_Rs2OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
        "X1_indices = sentences_to_indices(X1, word_to_index, max_len=5)\n",
        "print(f\"X1 = {X1}\")\n",
        "print(f\"X1_indices =\\n {X1_indices}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qWv25w9tz-m",
        "outputId": "1295294a-5faa-4857-ffbe-c87c7ae195e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
            "X1_indices =\n",
            " [[155345. 225122.      0.      0.      0.]\n",
            " [220930. 286375.  69714.      0.      0.]\n",
            " [151204. 192973. 302254. 151349. 394475.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building Embedding Layer**\n",
        "\n",
        "We will build the `Embedding()` layer in Keras using pre-trained word vectors. \n",
        "\n",
        "* The embedding layer takes as input a list of word indices\n",
        "    * `sentences_to_indices()` creates these word indices\n",
        "* The embedding layer will return the word embeddings for a sentence \n",
        "\n",
        "\n",
        "We will implement `pretrained_embedding_layer()` by following these steps:\n",
        "\n",
        "1. Initializing the embedding matrix as a numpy array of zeros\n",
        "    * The embedding matrix has a row for each unique word in the vocabulary\n",
        "        * There is one additional row to handle \"unknown\" words\n",
        "        * So vocab_size is the number of unique words plus one\n",
        "    * Each row will store the vector representation of one word \n",
        "        *  One row may be 50 positions long if using GloVe word vectors\n",
        "    * `emb_dim` represents the length of a word embedding\n",
        "2. Filling in each row of the embedding matrix with the vector representation of a word\n",
        "    * Each word in `word_to_index` is a string\n",
        "    * word_to_vec_map is a dictionary where the keys are strings and the values are the word vectors\n",
        "3. Defining the Keras embedding layer \n",
        "    * Using [Embedding()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) \n",
        "    * The input dimension is equal to the vocabulary length (number of unique words plus one)\n",
        "    * The output dimension is equal to the number of positions in a word embedding\n",
        "    * Making this layer's embeddings fixed\n",
        "        * If we are to set `trainable = True`, then it will allow the optimization algorithm to modify the values of the word embeddings\n",
        "        * we don't want the model to modify the word embeddings\n",
        "4. Setting the embedding weights to be equal to the embedding matrix\n",
        " "
      ],
      "metadata": {
        "id": "4cMQOTzC5FKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrained embedding layer\n",
        "\n",
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "      \n",
        "    # Arguments:\n",
        "    # word_to_vec_map -> dictionary mapping words to their GloVe vector representation\n",
        "    # word_to_index -> dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    # Returns:\n",
        "    # embedding_layer -> pretrained layer Keras instance\n",
        "    \n",
        "    \n",
        "    vocab_size = len(word_to_index) + 1              # adding 1 to fit Keras embedding (requirement)\n",
        "    any_word = list(word_to_vec_map.keys())[0]\n",
        "    emb_dim = word_to_vec_map[any_word].shape[0]     # define dimensionality of your GloVe word vectors (= 50)\n",
        "      \n",
        "    # Step 1: Initialize the embedding matrix as a numpy array of zeros\n",
        "    emb_matrix = np.zeros((vocab_size, emb_dim))\n",
        "    \n",
        "    # Step 2: Set each row \"idx\" of the embedding matrix to be the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
        "\n",
        "    # Step 3: Define Keras embedding layer with the correct input and output sizes. Make it non-trainable\n",
        "    embedding_layer = Embedding(vocab_size , emb_dim , trainable = False)\n",
        "\n",
        "    # Step 4: Build the embedding layer, it is required before setting the weights of the embedding layer \n",
        "    embedding_layer.build((None,)) # Do not modify the \"None\".  This line of code is complete as-is.\n",
        "    \n",
        "    # Set the weights of the embedding layer to the embedding matrix. Our layer is now pretrained.\n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "    \n",
        "    return embedding_layer"
      ],
      "metadata": {
        "id": "LflWscPw7GdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "print(f\"weights[0][1][1] = {embedding_layer.get_weights()[0][1][1]}\")\n",
        "print(f\"Input_dim  {embedding_layer.input_dim}\")\n",
        "print(f\"Output_dim  {embedding_layer.output_dim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcGhD4T78_DI",
        "outputId": "ec4766fa-ec5a-491d-ccfc-7f89e5fa90fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights[0][1][1] = 0.39030998945236206\n",
            "Input_dim  400001\n",
            "Output_dim  50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building The Emojifier**\n",
        "\n",
        "We will implement `Emojify_V2()` function that builds a Keras graph of the architecture. \n",
        "\n",
        "* The model takes as input an array of sentences of shape (`m`, `max_len`, ) defined by `input_shape` \n",
        "* The model outputs a softmax probability vector of shape (`m`, `C = 5`) \n",
        "\n",
        "* We will use the following Keras layers:\n",
        "    * [Input()](https://www.tensorflow.org/api_docs/python/tf/keras/Input)\n",
        "        * Set the `shape` and `dtype` parameters\n",
        "        * The inputs are integers so we can specify the data type as a string, 'int32'.\n",
        "    * [LSTM()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
        "        * Set the `units` and `return_sequences` parameters\n",
        "    * [Dropout()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)\n",
        "        * Set the `rate` parameter\n",
        "    * [Dense()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
        "        * Set the `units` \n",
        "       \n",
        "    * [Activation()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation)\n",
        "        * We can pass in the activation of our choice as a lowercase string\n",
        "    * [Model()](https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n",
        "        * Set `inputs` and `outputs`\n",
        "\n",
        "\n",
        "\n",
        "These Keras layers return an object and we will feed in the outputs of the previous layer as the input arguments to that object. The returned object can be created and called in the same line.\n",
        "\n",
        "The `embedding_layer` that is returned by `pretrained_embedding_layer` is a layer object that can be called as a function, passing in a single argument (sentence indices)."
      ],
      "metadata": {
        "id": "OLEiWU34-FRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Emojify V2\n",
        "\n",
        "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
        "    \n",
        "    # Arguments:\n",
        "    # input_shape -> shape of the input usually (max_len,)\n",
        "    # word_to_vec_map -> dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    # word_to_index -> dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    # Returns:\n",
        "    # model -> a model instance in Keras\n",
        "  \n",
        "    \n",
        "    # Define sentence_indices as the input of the graph\n",
        "    # It should be of shape input_shape and dtype 'int32' as it contains indices, which are integers\n",
        "    sentence_indices = Input(shape = input_shape, dtype = np.int32)\n",
        "    \n",
        "    # Create the embedding layer pretrained with GloVe Vectors \n",
        "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    \n",
        "    # Propagate sentence_indices through embedding layer\n",
        "    embeddings = embedding_layer(sentence_indices)   \n",
        "    \n",
        "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
        "    # The returned output should be a batch of sequences\n",
        "    X = LSTM(128, return_sequences=True)(embeddings)\n",
        "    \n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(0.5)(X)\n",
        "    \n",
        "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
        "    # The returned output should be a single hidden state not a batch of sequences\n",
        "    X = LSTM(128)(X)\n",
        "    \n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(0.5)(X)\n",
        "   \n",
        "    # Propagate X through a Dense layer with 5 units\n",
        "    X = Dense(5)(X)\n",
        "   \n",
        "    # Add a softmax activation\n",
        "    X = Activation(\"softmax\")(X)\n",
        "    \n",
        "    # Create Model instance which converts sentence_indices into X\n",
        "    model = Model(inputs=sentence_indices, outputs=X)\n",
        "    \n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "gHZSLSGJA2pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the model**  \n",
        "\n",
        "* Because all sentences in the dataset are less than 10 words so `max_len = 10` was chosen.  \n",
        "* Our architecture uses 20,223,927 parameters of which 20,000,050 (the word embeddings) are non-trainable with the remaining 223,877 being trainable \n",
        "* Because our vocabulary size has 400,001 words (with valid indices from 0 to 400,000) there are 400,001\\*50 = 20,000,050 non-trainable parameters "
      ],
      "metadata": {
        "id": "IT4XWKbIDUFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hn1Y8HFDsOt",
        "outputId": "a4a8ba6d-e1b9-482f-9e2f-e4472c28b05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 10)]              0         \n",
            "                                                                 \n",
            " embedding_4 (Embedding)     (None, 10, 50)            20000050  \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 10, 128)           91648     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 10, 128)           0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 5)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,223,927\n",
            "Trainable params: 223,877\n",
            "Non-trainable params: 20,000,050\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile The Model** \n",
        "\n",
        "After creating model in Keras, we need to compile it and define what loss, optimizer and metrics we want to use. Compiling our model using `categorical_crossentropy` loss, `adam` optimizer and `['accuracy']` metrics:"
      ],
      "metadata": {
        "id": "FB84nVeMD5nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-8u9RHiZEGbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train The Model** \n",
        "\n",
        "It's time to train the model! The Emojifier-V2 `model` takes as input an array of shape (`m`, `max_len`) and outputs probability vectors of shape (`m`, `number of classes`). Thus, we have to convert X_train (array of sentences as strings) to X_train_indices (array of sentences as list of word indices) and Y_train (labels as indices) to Y_train_oh (labels as one-hot vectors)."
      ],
      "metadata": {
        "id": "RGKHwMy6EYZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
      ],
      "metadata": {
        "id": "MM2VV2hzEkGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykT50xIqE1si",
        "outputId": "8ff07550-4e0b-472f-d464-6495b1d5b0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - 4s 29ms/step - loss: 1.5935 - accuracy: 0.2652\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 1.5356 - accuracy: 0.3409\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 1.4696 - accuracy: 0.3561\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 1.4174 - accuracy: 0.4394\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3545 - accuracy: 0.4621\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 1.2459 - accuracy: 0.5682\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 1.1046 - accuracy: 0.6439\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.0004 - accuracy: 0.6288\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.8659 - accuracy: 0.6818\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.7721 - accuracy: 0.7273\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.6822 - accuracy: 0.7273\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.6094 - accuracy: 0.7727\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.6503 - accuracy: 0.7727\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.5909 - accuracy: 0.7879\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.4548 - accuracy: 0.8561\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.5185 - accuracy: 0.8030\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.4382 - accuracy: 0.8561\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.4764 - accuracy: 0.8333\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.3443 - accuracy: 0.9091\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.2929 - accuracy: 0.9015\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.2631 - accuracy: 0.9167\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.3089 - accuracy: 0.9167\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.2175 - accuracy: 0.9015\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.1784 - accuracy: 0.9470\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.2146 - accuracy: 0.9091\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.1764 - accuracy: 0.9394\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.1953 - accuracy: 0.9394\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.1982 - accuracy: 0.9318\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.1499 - accuracy: 0.9470\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.1420 - accuracy: 0.9545\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.1662 - accuracy: 0.9394\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.1561 - accuracy: 0.9318\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.2019 - accuracy: 0.9242\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.1258 - accuracy: 0.9470\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.1534 - accuracy: 0.9318\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.1055 - accuracy: 0.9697\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.2315 - accuracy: 0.9242\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.2049 - accuracy: 0.9167\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.1603 - accuracy: 0.9394\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.1199 - accuracy: 0.9697\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.0996 - accuracy: 0.9773\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0757 - accuracy: 0.9697\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0493 - accuracy: 0.9848\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.0457 - accuracy: 0.9924\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0324 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.0220 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.0160 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0126 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f062ec0c750>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
        "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
        "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
        "print()\n",
        "print(f\"Test accuracy = {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RavsBGHfFCUM",
        "outputId": "8d57047a-b853-44b2-cc5d-c40e68cd3ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 12ms/step - loss: 0.4503 - accuracy: 0.8750\n",
            "\n",
            "Test accuracy = 0.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see the mislabelled examples\n",
        "C = 5\n",
        "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
        "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
        "pred = model.predict(X_test_indices)\n",
        "for i in range(len(X_test)):\n",
        "    x = X_test_indices\n",
        "    num = np.argmax(pred[i])\n",
        "    if(num != Y_test[i]):\n",
        "        print(f\"Expected emoji: {label_to_emoji(Y_test[i])}  prediction:  {X_test[i]} {label_to_emoji(num).strip()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1CH3hwfFZWo",
        "outputId": "6f66ffab-80c1-496a-f94a-909d62453c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected emoji: üòû  prediction:  work is hard\t üòÑ\n",
            "Expected emoji: üòû  prediction:  This girl is messing with me\t ‚ù§Ô∏è\n",
            "Expected emoji: üòû  prediction:  work is horrible\t üòÑ\n",
            "Expected emoji: ‚ù§Ô∏è  prediction:  I love taking breaks\t üòû\n",
            "Expected emoji: üòÑ  prediction:  you brighten my day\t ‚ù§Ô∏è\n",
            "Expected emoji: üòû  prediction:  she is a bully\t ‚ù§Ô∏è\n",
            "Expected emoji: üòÑ  prediction:  will you be my valentine\t ‚ù§Ô∏è\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the sentence below to see prediction. Make sure all the words are in the Glove embeddings.  \n",
        "x_test = np.array([\"I am not happy\"])\n",
        "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
        "print(f\"{x_test[0]} {label_to_emoji(np.argmax(model.predict(X_test_indices)))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QkyIaIWGSgK",
        "outputId": "4cffb4a4-dc6d-4314-c2d5-e4b005e4da2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am not happy üòû\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model considers the order of words. If the training set were larger then LSTM model would be much better at understanding more complex sentences."
      ],
      "metadata": {
        "id": "mK5xTiS0Hsv1"
      }
    }
  ]
}