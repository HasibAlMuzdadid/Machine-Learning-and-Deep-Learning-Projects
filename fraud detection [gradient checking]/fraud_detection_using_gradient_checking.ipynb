{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fraud detection using gradient checking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWzM1nmwpOhMONPVhwCIer",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HasibAlMuzdadid/Machine-Learning-and-Deep-Learning-Projects/blob/main/fraud%20detection%20%5Bgradient%20checking%5D/fraud_detection_using_gradient_checking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Authenticity using Gradient Checking**"
      ],
      "metadata": {
        "id": "uha0Tg5HeqcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are part of a team working to make mobile payments available globally, and are asked to build a deep learning model to detect fraud--whenever someone makes a payment, you want to see if the payment might be fraudulent, such as if the user's account has been taken over by a hacker.\n",
        "\n",
        "\n",
        "We already know that backpropagation is quite challenging to implement, and sometimes has bugs. Because this is a mission-critical application, your company's CEO wants to be really certain that your implementation of backpropagation is correct. Your CEO says, *Give me proof that your backpropagation is actually working!*. To give this reassurance, you are going to use **gradient checking**.\n"
      ],
      "metadata": {
        "id": "wcH--7vBd30w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZDZfsvRattD"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "def sigmoid(x):\n",
        "  s = 1/(1+np.exp(-x))\n",
        "  return s\n",
        "\n",
        "def relu(x):\n",
        "  s = np.maximum(0,x)  \n",
        "  return s"
      ],
      "metadata": {
        "id": "uq5n49KQa7HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dictionary_to_vector(parameters):\n",
        "\n",
        "  keys = []\n",
        "  count = 0\n",
        "  for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"]:\n",
        "        \n",
        "    # flatten parameter\n",
        "    new_vector = np.reshape(parameters[key], (-1,1))\n",
        "    keys = keys + [key]*new_vector.shape[0]\n",
        "        \n",
        "    if count == 0:\n",
        "      theta = new_vector\n",
        "    else:\n",
        "      theta = np.concatenate((theta, new_vector), axis=0)\n",
        "    \n",
        "    count = count + 1\n",
        "\n",
        "  return theta, keys\n",
        "\n",
        "\n",
        "def vector_to_dictionary(theta):\n",
        "\n",
        "  parameters = {}\n",
        "  parameters[\"W1\"] = theta[:20].reshape((5,4))\n",
        "  parameters[\"b1\"] = theta[20:25].reshape((5,1))\n",
        "  parameters[\"W2\"] = theta[25:40].reshape((3,5))\n",
        "  parameters[\"b2\"] = theta[40:43].reshape((3,1))\n",
        "  parameters[\"W3\"] = theta[43:46].reshape((1,3))\n",
        "  parameters[\"b3\"] = theta[46:47].reshape((1,1))\n",
        "\n",
        "  return parameters\n",
        "\n",
        "\n",
        "def gradients_to_vector(gradients):\n",
        " \n",
        "  count = 0\n",
        "  for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\"]:\n",
        "    # flatten parameter\n",
        "    new_vector = np.reshape(gradients[key], (-1,1))\n",
        "        \n",
        "    if count == 0:\n",
        "      theta = new_vector\n",
        "    else:\n",
        "      theta = np.concatenate((theta, new_vector), axis=0)\n",
        "    \n",
        "    count = count + 1\n",
        "\n",
        "  return theta\n",
        "\n",
        "def gradient_check_n_test_case(): \n",
        " \n",
        "  x = np.random.randn(4,3)\n",
        "  y = np.array([1, 1, 0])\n",
        "  W1 = np.random.randn(5,4) \n",
        "  b1 = np.random.randn(5,1) \n",
        "  W2 = np.random.randn(3,5) \n",
        "  b2 = np.random.randn(3,1) \n",
        "  W3 = np.random.randn(1,3) \n",
        "  b3 = np.random.randn(1,1) \n",
        "  parameters = {\"W1\": W1,\n",
        "                \"b1\": b1,\n",
        "                \"W2\": W2,\n",
        "                \"b2\": b2,\n",
        "                \"W3\": W3,\n",
        "                \"b3\": b3}\n",
        "\n",
        "    \n",
        "  return x, y, parameters  "
      ],
      "metadata": {
        "id": "cQZKpxCbbH7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Dimensional Gradient Checking**"
      ],
      "metadata": {
        "id": "BIiw3G4zY4ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward propagation\n",
        "\n",
        "def forward_propagation(x, theta):\n",
        "\n",
        "  J = theta * x\n",
        "  \n",
        "    \n",
        "  return J"
      ],
      "metadata": {
        "id": "diY1DAu_b4Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation\n",
        "\n",
        "def backward_propagation(x, theta):\n",
        "  \n",
        "  dtheta = x\n",
        "    \n",
        "\n",
        "  return dtheta"
      ],
      "metadata": {
        "id": "n00kdAnYXdj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient check\n",
        "\n",
        "def gradient_check(x, theta, epsilon=1e-7, print_msg=False):\n",
        "   \n",
        "  # Compute gradapprox\n",
        "  thetaplus = theta + epsilon                               # Step 1\n",
        "  thetaminus = theta - epsilon                              # Step 2\n",
        "  J_plus = forward_propagation(x, thetaplus)                # Step 3\n",
        "  J_minus = forward_propagation(x, thetaminus)              # Step 4\n",
        "  gradapprox = (J_plus - J_minus) / (2*epsilon)             # Step 5\n",
        "    \n",
        "    \n",
        "  # Check if gradapprox is close enough to the output of backward_propagation()\n",
        "  grad = backward_propagation(x, theta)\n",
        "    \n",
        "  numerator = np.linalg.norm(grad-gradapprox)                                  # Step 1'\n",
        "  denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)              # Step 2'\n",
        "  difference = numerator / denominator                                         # Step 3'\n",
        "    \n",
        "  if print_msg:\n",
        "    if difference > 2e-7:\n",
        "      print (\"\\033[91m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
        "    else:\n",
        "      print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
        "    \n",
        "  return difference"
      ],
      "metadata": {
        "id": "Qn3VDg9rXyIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, theta = 2, 4\n",
        "difference = gradient_check(x, theta, print_msg=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRBPF-i9bX1R",
        "outputId": "a555eeee-b315-4d14-9738-e3c341ac6a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mYour backward propagation works perfectly fine! difference = 2.919335883291695e-10\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N-Dimensional Gradient Checking**"
      ],
      "metadata": {
        "id": "IJbYkfcTY_Pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation_n(X, Y, parameters):\n",
        "   \n",
        "  # retrieve parameters\n",
        "  m = X.shape[1]\n",
        "  W1 = parameters[\"W1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "  W3 = parameters[\"W3\"]\n",
        "  b3 = parameters[\"b3\"]\n",
        "\n",
        "  # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
        "  Z1 = np.dot(W1, X) + b1\n",
        "  A1 = relu(Z1)\n",
        "  Z2 = np.dot(W2, A1) + b2\n",
        "  A2 = relu(Z2)\n",
        "  Z3 = np.dot(W3, A2) + b3\n",
        "  A3 = sigmoid(Z3)\n",
        "\n",
        "  # Cost\n",
        "  log_probs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
        "  cost = 1. / m * np.sum(log_probs)\n",
        "    \n",
        "  cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
        "    \n",
        "  return cost, cache"
      ],
      "metadata": {
        "id": "-cvDVLOYZE2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation_n(X, Y, cache):\n",
        "\n",
        "  m = X.shape[1]\n",
        "  (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "    \n",
        "  dZ3 = A3 - Y\n",
        "  dW3 = 1. / m * np.dot(dZ3, A2.T)\n",
        "  db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
        "    \n",
        "  dA2 = np.dot(W3.T, dZ3)\n",
        "  dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
        "  dW2 = 1. / m * np.dot(dZ2, A1.T) * 2                          # error 1\n",
        "  db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    \n",
        "  dA1 = np.dot(W2.T, dZ2)\n",
        "  dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
        "  dW1 = 1. / m * np.dot(dZ1, X.T)\n",
        "  db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True)             # error 2\n",
        "    \n",
        "  gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
        "               \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
        "               \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "    \n",
        "  return gradients"
      ],
      "metadata": {
        "id": "XN5Pzm-fZb8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: gradient_check_n\n",
        "\n",
        "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7, print_msg=False):\n",
        "    \n",
        "  # Set-up variables\n",
        "  parameters_values, _ = dictionary_to_vector(parameters)\n",
        "    \n",
        "  grad = gradients_to_vector(gradients)\n",
        "  num_parameters = parameters_values.shape[0]\n",
        "  J_plus = np.zeros((num_parameters, 1))\n",
        "  J_minus = np.zeros((num_parameters, 1))\n",
        "  gradapprox = np.zeros((num_parameters, 1))\n",
        "    \n",
        "  # Compute gradapprox\n",
        "  for i in range(num_parameters):\n",
        "        \n",
        "    # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
        "    # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
        "\n",
        "    thetaplus = parameters_values.copy()                                                   # Step 1\n",
        "    thetaplus[i][0] = thetaplus[i][0] + epsilon                                            # Step 2\n",
        "    J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))            # Step 3\n",
        "        \n",
        "        \n",
        "    # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
        "    thetaminus = parameters_values.copy()                                              # Step 1\n",
        "    thetaminus[i][0] = thetaminus[i][0] - epsilon                                      # Step 2        \n",
        "    J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))      # Step 3\n",
        "        \n",
        "    # Compute gradapprox[i]\n",
        "    gradapprox[i] = (J_plus[i] - J_minus[i]) / (2*epsilon)\n",
        "    \n",
        "  # Compare gradapprox to backward propagation gradients by computing difference.\n",
        "  numerator = np.linalg.norm(grad-gradapprox)                                         # Step 1\n",
        "  denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                     # Step 2\n",
        "  difference = numerator / denominator                                                # Step 3\n",
        "    \n",
        "  if print_msg:\n",
        "    if difference > 2e-7:\n",
        "      print (\"\\033[91m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
        "    else:\n",
        "      print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
        "\n",
        "  return difference"
      ],
      "metadata": {
        "id": "aoBcsBKNZs9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y, parameters = gradient_check_n_test_case()\n",
        "\n",
        "cost, cache = forward_propagation_n(X, Y, parameters)\n",
        "gradients = backward_propagation_n(X, Y, cache)\n",
        "difference = gradient_check_n(parameters, gradients, X, Y, 1e-7, True)\n",
        "expected_values = [0.2850931567761623, 1.1890913024229996e-07]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejaTvIvGbIlt",
        "outputId": "61ea47d8-26c5-45fe-d78e-8123eb05c8c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[91mThere is a mistake in the backward propagation! difference = 8.615988854416132e-05\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}