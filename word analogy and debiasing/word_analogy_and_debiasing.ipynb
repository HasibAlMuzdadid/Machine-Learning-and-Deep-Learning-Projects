{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BiUnAlVqDiYH7NA487_5p8xLw5x5ka9X",
      "authorship_tag": "ABX9TyMObps8KpAOQizAnOW/ZkKb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HasibAlMuzdadid/Machine-Learning-and-Deep-Learning-Projects/blob/main/word%20analogy%20and%20debiasing/word_analogy_and_debiasing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Analogy**\n",
        "\n",
        "Word embeddings are very computationally expensive to train and most ML practitioners load a pre-trained set of embeddings. Here we'll try our hand at loading, measuring similarity between and modifying pre-trained embeddings. "
      ],
      "metadata": {
        "id": "9yllKkrkJ_HE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrfTXhP9Jl2_"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "\n",
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        \n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "            \n",
        "    return words, word_to_vec_map"
      ],
      "metadata": {
        "id": "GavhV1UdN6dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use 50-dimensional [GloVe vectors](https://nlp.stanford.edu/projects/glove/) to represent words."
      ],
      "metadata": {
        "id": "sOY5bvuiNKK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading word vectors\n",
        "\n",
        "words, word_to_vec_map = read_glove_vecs(\"/content/drive/MyDrive/colab files/glove.6B.50d.txt\")"
      ],
      "metadata": {
        "id": "kHulzwseNOJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `words`: set of words in the vocabulary.\n",
        "- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation.\n"
      ],
      "metadata": {
        "id": "2WkLv4ykOWzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding Vectors Versus One-Hot Vectors**\n",
        "\n",
        "One-hot vectors don't do a good job of capturing the level of similarity between words. This is because every one-hot vector has the same Euclidean distance from any other one-hot vector.\n",
        "\n",
        "Embedding vectors, such as GloVe vectors, provide much more useful information about the meaning of individual words.  \n",
        "\n",
        "\n",
        "**Cosine Similarity**\n",
        "\n",
        "To measure the similarity between two words, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n",
        "\n",
        "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta) $$\n",
        "\n",
        "* $u \\cdot v$ is the dot product (or inner product) of two vectors\n",
        "* $||u||_2$ is the norm (or length) of the vector $u$\n",
        "and defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$\n",
        "\n",
        "* $\\theta$ is the angle between $u$ and $v$. \n",
        "* The cosine similarity depends on the angle between $u$ and $v$. \n",
        "    * If $u$ and $v$ are very similar, their cosine similarity will be close to 1.\n",
        "    * If they are dissimilar, the cosine similarity will take a smaller value. \n"
      ],
      "metadata": {
        "id": "oKrXUT5nOaRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cosine similarity\n",
        "\n",
        "def cosine_similarity(u, v):\n",
        "        \n",
        "        # Arguments:\n",
        "        # u -> a word vector of shape (n,)          \n",
        "        # v -> a word vector of shape (n,)\n",
        "\n",
        "        # Returns:\n",
        "        # cosine_similarity -> the cosine similarity between u and v \n",
        "\n",
        "    \n",
        "    # Special case. Consider the case u = [0, 0], v=[0, 0]\n",
        "    if np.all(u == v):\n",
        "        return 1\n",
        "    \n",
        "    # Compute the dot product between u and v \n",
        "    dot = np.dot(u, v) \n",
        "   \n",
        "    # Compute the L2 norm of u \n",
        "    norm_u = np.sqrt(np.sum(u**2))\n",
        "    \n",
        "    # Compute the L2 norm of v \n",
        "    norm_v = np.sqrt(np.sum(v**2))\n",
        "    \n",
        "    # Avoid division by 0\n",
        "    if np.isclose(norm_u * norm_v, 0, atol=1e-32):\n",
        "        return 0\n",
        "    \n",
        "    # Compute the cosine similarity \n",
        "    cosine_similarity = dot/(norm_u * norm_v)\n",
        "\n",
        "    \n",
        "    return cosine_similarity"
      ],
      "metadata": {
        "id": "HDutG9qXPynP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Analogy**\n",
        "\n",
        "In the word analogy task, complete this sentence:  \n",
        "    <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. \n",
        "\n",
        "An example is:  <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. \n",
        "\n",
        "We're trying to find a word *d* such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner:   \n",
        "$e_b - e_a \\approx e_d - e_c$.\n",
        "Measuring the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity "
      ],
      "metadata": {
        "id": "XFMq69HpRRnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# complete analogy\n",
        "\n",
        "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
        "    \n",
        "    # Arguments:\n",
        "    # word_a -> a word, string\n",
        "    # word_b -> a word, string\n",
        "    # word_c -> a word, string\n",
        "    # word_to_vec_map -> dictionary that maps words to their corresponding vectors \n",
        "    \n",
        "    # Returns:\n",
        "    # best_word ->  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
        "\n",
        "    \n",
        "    # convert words to lowercase\n",
        "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
        "    \n",
        "    # Get the word embeddings e_a, e_b and e_c \n",
        "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
        "\n",
        "    \n",
        "    words = word_to_vec_map.keys()\n",
        "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
        "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
        "    \n",
        "    # loop over the whole word vector set\n",
        "    for w in words:   \n",
        "        # to avoid best_word being one the input words, skip the input word_c\n",
        "        # skip word_c from query\n",
        "        if w == word_c:\n",
        "            continue\n",
        "        \n",
        "\n",
        "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  \n",
        "        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n",
        "        \n",
        "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
        "        # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word \n",
        "        if cosine_sim > max_cosine_sim:\n",
        "            max_cosine_sim = cosine_sim\n",
        "            best_word = w\n",
        "\n",
        "        \n",
        "    return best_word"
      ],
      "metadata": {
        "id": "APPMV2tyRh-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
        "for triad in triads_to_try:\n",
        "    # print('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad, word_to_vec_map)))\n",
        "    print(f\"{triad[0]} -> {triad[1]} :: {triad[2]} -> {complete_analogy(triad[0],triad[1],triad[2], word_to_vec_map)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtgElbfpTQOg",
        "outputId": "a791d7cf-55ee-4b9f-cd30-86f31a225d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "italy -> italian :: spain -> spanish\n",
            "india -> delhi :: japan -> tokyo\n",
            "man -> woman :: boy -> girl\n",
            "small -> smaller :: large -> smaller\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity is a relatively simple and intuitive, yet powerful method we can use to capture nuanced relationships between words."
      ],
      "metadata": {
        "id": "RnI8ucJ6R1nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Debiasing Word Vectors** \n",
        "\n",
        "We'll examine gender biases that can be reflected in a word embedding and explore algorithms for reducing the bias. In addition to learning about the topic of debiasing which will also help hone our intuition about what word vectors are doing. \n",
        "\n",
        "First, discovering how the GloVe word embeddings relate to gender. We'll begin by computing a vector $g = e_{woman}-e_{man}$, where $e_{woman}$ represents the word vector corresponding to the word *woman*, and $e_{man}$ corresponds to the word vector corresponding to the word *man*. The resulting vector $g$ roughly encodes the concept of \"gender\". \n",
        "\n",
        "We might get a more accurate representation if we compute $g_1 = e_{mother}-e_{father}$, $g_2 = e_{girl}-e_{boy}$ etc and average over them but just using $e_{woman}-e_{man}$ will give good enough results for now.\n"
      ],
      "metadata": {
        "id": "pSB-xkqkSHGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = word_to_vec_map[\"woman\"] - word_to_vec_map[\"man\"]\n",
        "print(g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7vMTfe9UHCf",
        "outputId": "7d540a13-0a25-4b93-bd87-aa1d6177dad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.087144    0.2182     -0.40986    -0.03922    -0.1032      0.94165\n",
            " -0.06042     0.32988     0.46144    -0.35962     0.31102    -0.86824\n",
            "  0.96006     0.01073     0.24337     0.08193    -1.02722    -0.21122\n",
            "  0.695044   -0.00222     0.29106     0.5053     -0.099454    0.40445\n",
            "  0.30181     0.1355     -0.0606     -0.07131    -0.19245    -0.06115\n",
            " -0.3204      0.07165    -0.13337    -0.25068714 -0.14293    -0.224957\n",
            " -0.149       0.048882    0.12191    -0.27362    -0.165476   -0.20426\n",
            "  0.54376    -0.271425   -0.10245    -0.32108     0.2516     -0.33455\n",
            " -0.04371     0.01258   ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering the cosine similarity of different words with $g$. What does a positive value of similarity mean versus a negative cosine similarity? "
      ],
      "metadata": {
        "id": "g2PpYoPXUaWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"List of names and their similarities with constructed vector:\")\n",
        "\n",
        "# girls and boys name\n",
        "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
        "\n",
        "for w in name_list:\n",
        "    print(f\"{w} : {cosine_similarity(word_to_vec_map[w], g)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goXkQeLQR-PD",
        "outputId": "c3c5303b-a2ff-4c50-f8cb-12fca692a5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of names and their similarities with constructed vector:\n",
            "john : -0.23163356145973724\n",
            "marie : 0.315597935396073\n",
            "sophie : 0.31868789859418784\n",
            "ronaldo : -0.31244796850329437\n",
            "priya : 0.17632041839009402\n",
            "rahul : -0.16915471039231716\n",
            "danielle : 0.24393299216283895\n",
            "reza : -0.07930429672199553\n",
            "katy : 0.2831068659572615\n",
            "yasmin : 0.23313857767928758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see, female first names tend to have a positive cosine similarity with our constructed vector $g$, while male first names tend to have a negative cosine similarity. This is not surprising and the result seems acceptable. \n",
        "\n",
        "Now trying with some other words:"
      ],
      "metadata": {
        "id": "leMnpB0VVhEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Other words and their similarities:\")\n",
        "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
        "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
        "for w in word_list:\n",
        "    print(f\"{w} : {cosine_similarity(word_to_vec_map[w], g)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CuZfNw7Vntz",
        "outputId": "9cabe3ef-1048-4d66-faab-f5cd1b1ef2ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Other words and their similarities:\n",
            "lipstick : 0.2769191625638267\n",
            "guns : -0.1888485567898898\n",
            "science : -0.06082906540929701\n",
            "arts : 0.008189312385880337\n",
            "literature : 0.06472504433459932\n",
            "warrior : -0.20920164641125288\n",
            "doctor : 0.11895289410935041\n",
            "tree : -0.07089399175478091\n",
            "receptionist : 0.33077941750593737\n",
            "technology : -0.13193732447554302\n",
            "fashion : 0.03563894625772699\n",
            "teacher : 0.17920923431825664\n",
            "engineer : -0.0803928049452407\n",
            "pilot : 0.0010764498991916937\n",
            "computer : -0.10330358873850498\n",
            "singer : 0.1850051813649629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is astonishing how these results reflect certain unhealthy gender stereotypes. We see “computer” is negative and is closer in value to male first names while “literature” is positive and is closer to female first names.\n",
        "\n",
        "We will reduce the bias of these vectors using an algorithm due to [Boliukbasi et al., 2016](https://arxiv.org/abs/1607.06520). Some word pairs such as \"actor\"/\"actress\" or \"grandmother\"/\"grandfather\" should remain gender-specific, while other words such as \"receptionist\" or \"technology\" should be neutralized i.e. not be gender-related. We'll have to treat these two types of words differently when debiasing.\n",
        "\n",
        "\n",
        "**Neutralize Bias for Non-Gender Specific Words** \n",
        "\n",
        "If we're using a 50-dimensional word embedding, the 50 dimensional space can be split into two parts: The bias-direction $g$ and the remaining 49 dimensions which is called $g_{\\perp}$ here. In linear algebra, we say that the 49-dimensional $g_{\\perp}$ is perpendicular (or \"orthogonal\") to $g$. The neutralization step takes a vector such as $e_{receptionist}$ and zeros out the component in the direction of $g$, giving us $e_{receptionist}^{debiased}$. \n",
        "\n",
        "\n",
        "**Implementing Neutralize** \n",
        "\n",
        "To remove the bias of words such as \"receptionist\" or \"scientist.\"\n",
        "\n",
        "Given an input embedding $e$, we can use the following formulas to compute $e^{debiased}$: \n",
        "\n",
        "$$e^{bias\\_component} = \\frac{e \\cdot g}{||g||_2^2} * g$$\n",
        "$$e^{debiased} = e - e^{bias\\_component}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "EypHoS0sZXFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def neutralize(word, g, word_to_vec_map):\n",
        "    \n",
        "        # Arguments:\n",
        "        # word -> string indicating the word to debias\n",
        "        # g -> numpy-array of shape (50,), corresponding to the bias axis (such as gender)\n",
        "        # word_to_vec_map -> dictionary mapping words to their corresponding vectors\n",
        "    \n",
        "        # Returns:\n",
        "        # e_debiased -> neutralized word vector representation of the input \"word\"\n",
        "\n",
        "    \n",
        "    # Selecting word vector representation of \"word\" using word_to_vec_map\n",
        "    e = word_to_vec_map[word]\n",
        "    \n",
        "    # Computing e_biascomponent using the formula given above\n",
        "    e_biascomponent = np.dot(e, g)/np.sum(g**2) * g\n",
        " \n",
        "    # Neutralizing e by subtracting e_biascomponent from it \n",
        "    # e_debiased should be equal to its orthogonal projection\n",
        "    e_debiased = e - e_biascomponent\n",
        "\n",
        "    \n",
        "    return e_debiased"
      ],
      "metadata": {
        "id": "Wb-IOMEibPaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = \"receptionist\"\n",
        "print(f\"cosine similarity between {e} and g before neutralizing: {cosine_similarity(word_to_vec_map['receptionist'], g)}\")\n",
        "\n",
        "e_debiased = neutralize(\"receptionist\", g, word_to_vec_map)\n",
        "print(f\"cosine similarity between {e} and g after neutralizing: {cosine_similarity(e_debiased, g)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ6IBLHNbzaK",
        "outputId": "39deb7e3-1e7f-49ea-b324-b008d59ba951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine similarity between receptionist and g before neutralizing: 0.33077941750593737\n",
            "cosine similarity between receptionist and g after neutralizing: -2.6832242276243644e-17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equalization Algorithm for Gender-Specific Words**\n",
        "\n",
        "let's apply debiasing to word pairs such as \"actress\" and \"actor\". Equalization is applied to pairs of words that we might want to have differ only through the gender property.  Suppose that \"actress\" is closer to \"babysit\" than \"actor\". By applying neutralization to \"babysit\" we can reduce the gender stereotype associated with babysitting. But this still does not guarantee that \"actor\" and \"actress\" are equidistant from \"babysit\". The equalization algorithm takes care of this. \n",
        "\n",
        "The key idea behind equalization is to make sure that a particular pair of words are equidistant from the 49-dimensional $g_\\perp$. The equalization step also ensures that the two equalized steps are now the same distance from $e_{receptionist}^{debiased}$ or from any other work that has been neutralized. \n",
        "\n",
        "The derivation of the linear algebra to do this is a bit more complex. Here are the key equations: \n",
        "\n",
        "$$ \\mu = \\frac{e_{w1} + e_{w2}}{2}$$ \n",
        "\n",
        "$$ \\mu_{B} = \\frac {\\mu \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
        "$$ \n",
        "\n",
        "$$\\mu_{\\perp} = \\mu - \\mu_{B} $$\n",
        "\n",
        "$$ e_{w1B} = \\frac {e_{w1} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
        "$$ \n",
        "$$ e_{w2B} = \\frac {e_{w2} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
        "$$\n",
        "\n",
        "\n",
        "$$e_{w1B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w1B}} - \\mu_B} {||(e_{w1} - \\mu_{\\perp}) - \\mu_B||_2} $$\n",
        "\n",
        "\n",
        "$$e_{w2B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w2B}} - \\mu_B} {||(e_{w2} - \\mu_{\\perp}) - \\mu_B||_2} $$\n",
        "\n",
        "$$e_1 = e_{w1B}^{corrected} + \\mu_{\\perp} $$\n",
        "$$e_2 = e_{w2B}^{corrected} + \\mu_{\\perp} $$\n",
        "\n"
      ],
      "metadata": {
        "id": "qKDSyY-0iXId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def equalize(pair, bias_axis, word_to_vec_map):\n",
        "    \n",
        "    # Arguments:\n",
        "    # pair -> pair of strings of gender specific words to debias, e.g. (\"actress\", \"actor\") \n",
        "    # bias_axis -> numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender\n",
        "    # word_to_vec_map -> dictionary mapping words to their corresponding vectors\n",
        "    \n",
        "    # Returns:\n",
        "    # e_1 -> word vector corresponding to the first word\n",
        "    # e_2 -> word vector corresponding to the second word\n",
        "\n",
        "    \n",
        "    # Step 1: Selecting word vector representation of \"word\" using word_to_vec_map\n",
        "    w1, w2 = pair\n",
        "    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n",
        "    \n",
        "    # Step 2: Computing the mean of e_w1 and e_w2 \n",
        "    mu = (e_w1 + e_w2) / 2\n",
        "\n",
        "    # Step 3: Computing the projections of mu over the bias axis and the orthogonal axis \n",
        "    mu_B = np.dot(mu, bias_axis) * bias_axis /np.linalg.norm(bias_axis)**2\n",
        "    mu_orth = mu - mu_B\n",
        "\n",
        "    # Step 4: Using equations to compute e_w1B and e_w2B \n",
        "    e_w1B = np.dot(e_w1, bias_axis) * bias_axis/np.linalg.norm(bias_axis)**2\n",
        "    e_w2B = np.dot(e_w2, bias_axis) * bias_axis/np.linalg.norm(bias_axis)**2\n",
        "        \n",
        "    # Step 5: Adjusting the Bias part of e_w1B and e_w2B using the formulas given above \n",
        "    corrected_e_w1B = np.sqrt(abs(1 - np.linalg.norm(mu_orth)**2)) * (e_w1B - mu_B)/np.linalg.norm((e_w1B - mu_orth) - mu_B)\n",
        "    corrected_e_w2B = np.sqrt(abs(1 - np.linalg.norm(mu_orth)**2)) * (e_w2B - mu_B)/np.linalg.norm((e_w2B - mu_orth) - mu_B)\n",
        "    \n",
        "    # Step 6: Debiasing by equalizing e1 and e2 to the sum of their corrected projections \n",
        "    e1 = corrected_e_w1B + mu_orth\n",
        "    e2 = corrected_e_w2B + mu_orth\n",
        "                                                              \n",
        "    \n",
        "    return e1, e2"
      ],
      "metadata": {
        "id": "rX2XyxWgj0uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"cosine similarities before equalizing:\")\n",
        "print(f\"cosine_similarity(word_to_vec_map[\\'man\\'], gender) = {cosine_similarity(word_to_vec_map['man'], g)}\")\n",
        "print(f\"cosine_similarity(word_to_vec_map[\\'woman\\'], gender) = {cosine_similarity(word_to_vec_map['woman'], g)}\")\n",
        "print()\n",
        "e1, e2 = equalize((\"man\", \"woman\"), g, word_to_vec_map)\n",
        "print(\"cosine similarities after equalizing:\")\n",
        "print(f\"cosine_similarity(e1, gender) = {cosine_similarity(e1, g)}\")\n",
        "print(f\"cosine_similarity(e2, gender) = {cosine_similarity(e2, g)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdbVxfHQknLI",
        "outputId": "5ca4c27c-6e1e-4ec1-e98b-1e9d35ea5165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine similarities before equalizing:\n",
            "cosine_similarity(word_to_vec_map['man'], gender) = -0.11711095765336832\n",
            "cosine_similarity(word_to_vec_map['woman'], gender) = 0.35666618846270376\n",
            "\n",
            "cosine similarities after equalizing:\n",
            "cosine_similarity(e1, gender) = -0.23142942644908526\n",
            "cosine_similarity(e2, gender) = 0.2314294264490853\n"
          ]
        }
      ]
    }
  ]
}